{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cdd9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d915b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def _get_env_from_colab_or_os(key):\n",
    "    return os.getenv(key)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = _get_env_from_colab_or_os(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3421ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "converter = DocumentConverter()\n",
    "# --------------------------------------------------------------\n",
    "# Basic PDF extraction\n",
    "# --------------------------------------------------------------\n",
    "result = converter.convert(\"Constitucion_1978.md\")\n",
    "# result = converter.convert(\"Constituci√≥n 1978.pdf\")\n",
    "\n",
    "document = result.document\n",
    "markdown_output = document.export_to_markdown()\n",
    "json_output = document.export_to_dict()\n",
    "\n",
    "print(markdown_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79a668a",
   "metadata": {},
   "source": [
    "### CHUNKING (Docling-Hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ababbb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to use the hybrid chunking method from docling\n",
    "from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from docling.chunking import HybridChunker\n",
    "\n",
    "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "MAX_TOKENS = 128  \n",
    "\n",
    "tokenizer = HuggingFaceTokenizer(\n",
    "    tokenizer=AutoTokenizer.from_pretrained(EMBED_MODEL_ID),\n",
    "    max_tokens=MAX_TOKENS,  \n",
    ")\n",
    "\n",
    "chunker = HybridChunker(\n",
    "    tokenizer=tokenizer,\n",
    "    merge_peers=True, \n",
    ")\n",
    "\n",
    "chunk_iter = chunker.chunk(dl_doc=result.document)\n",
    "chunks = list(chunk_iter)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013e5073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To simplify the metadata\n",
    "for chunk in chunks:\n",
    "    simplified_meta = {\n",
    "        \"headings\": chunk.meta.headings,\n",
    "        \"filename\": chunk.meta.origin.filename  \n",
    "    }\n",
    "    chunk.meta = simplified_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b47e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking what the chunks and metadata look like\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i}: {chunk.text}\")\n",
    "    print(f\"Metadata: {chunk.meta}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feed331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the metadata to the text part to be embedded aswell\n",
    "from langchain.schema import Document\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=f\"Headings: {', '.join(chunk.meta['headings'])}\\nFilename: {chunk.meta['filename']}\\n\\nContent: {chunk.text}\",\n",
    "        metadata=chunk.meta\n",
    "    )\n",
    "    for chunk in chunks\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9fc0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import SupabaseVectorStore\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from supabase import create_client\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "TABLE_NAME = \"documents\"\n",
    "\n",
    "supabase_client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "# Create embedding model\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa9acc6",
   "metadata": {},
   "source": [
    "### Adding the chunks to Supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47c4f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_documents(chunks=None, texts=None, embeddings=None, supabase_client=None, table_name=None):\n",
    "    \"\"\"\n",
    "    Add documents to Supabase vector store.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of chunk objects with .text and .meta attributes (for complex documents)\n",
    "        texts: List of strings (for simple text documents)\n",
    "        embeddings: Embedding model instance\n",
    "        supabase_client: Initialized Supabase client\n",
    "        table_name: Name of the table in Supabase\n",
    "    \n",
    "    Note: Provide either chunks OR texts, not both.\n",
    "    \"\"\"\n",
    "    if chunks is not None and texts is not None:\n",
    "        raise ValueError(\"Provide either 'chunks' or 'texts', not both\")\n",
    "    \n",
    "    if chunks is None and texts is None:\n",
    "        raise ValueError(\"Must provide either 'chunks' or 'texts'\")\n",
    "    \n",
    "    # Create documents from chunks (complex documents with metadata)\n",
    "    if chunks is not None:\n",
    "        docs = [\n",
    "            Document(\n",
    "                page_content=f\"Headings: {', '.join(chunk.meta['headings'])}\\nFilename: {chunk.meta['filename']}\\n\\nContent: {chunk.text}\",\n",
    "                metadata=chunk.meta\n",
    "            )\n",
    "            for chunk in chunks\n",
    "        ]\n",
    "    \n",
    "    # Create documents from simple texts\n",
    "    else:\n",
    "        docs = [\n",
    "            Document(\n",
    "                page_content=text, \n",
    "                metadata={\"content_type\": \"text\"}\n",
    "            ) \n",
    "            for text in texts\n",
    "        ]\n",
    "\n",
    "    # Push to Supabase\n",
    "    SupabaseVectorStore.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embeddings,\n",
    "        client=supabase_client,\n",
    "        table_name=table_name,\n",
    "        query_name=\"match_documents\",\n",
    "    )\n",
    "    \n",
    "    print(f\"Successfully added {len(docs)} documents to Supabase\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dabc5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_documents()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
